 # An introduction to NeuralAnalysis Module
 
 ## General Notes

1. When I orginally designed this class it was largely based on dictionaries, but I find that dataframes are actually better for a lot of analyses, but this requires translation functions from my original dictionary structures (which all my plotting functions were writtened based on--although I often convert to dataframes within the plotting functions themselves, so maybe eventually I'll remove the dictionaries altogether). 

2. The goal will be to stack all analyses into large final dataframes, which are indexed by what I call the HashID. Since kilosort and phy always just give the same numbers between recordings I take the hash of the filename with the cluster number to generate a unique id for each neuron for each recording. This allows me to interact with multiple datasets while keeping track of unique ids. Haven't had any hash collisions yet.

3. With these caveats I save both the dictionaries and dataframes so either structure can be used for post-hoc analyses


## Inputs for the Class
 
 ### Neural Data
 This pipline is based specifically on kilosort/phy outputs. These include a series of numpy (.npy) files which are generated by kilosort and edited by phy. As long as phy is the final output then my `loadsp` function should load all data into a dictionary which I call `sp` for spike properties. Within the `ksanalysis.py` file all necessary keys can be found, but a few important ones include: `spikeTimes` are the list of spike occurrences in seconds, `clu` is the list of curated cluster ids for each spike, `cids` are the list of all possible ids after curation. 
 
 ### Stimulus Data
 Many *in vivo* experiments are based on recording neural activity in response to stimulus data. Stimulus data also needs to be loaded as a specific dictionary structure. Since I use Intan for recording data and stimuli, I use their python code for extracting stimulus data and generating this dictionary. If another data recording system is used the general format required is: `eventTimes['stim channel']['EventTime']: np.array` (of events), `eventTimes['stim channel']['Length']: np.array` (of lengths for each event), `eventTimes['stim channel']['TrialGroup']: np.array` of the different degree of stim (for example changing light orientation or changing pressure of stimuli) and `eventTimes['stim channel']['Stim']: str` ( the name of the stimulus for plotting). For detailed explanation see the `readme.md` in the `intan_helpers` folder.
 
 #### Intan_helpers Folder
 Includes functions to run intan data automatically along with my functions for prepping the stimulus data from the `.rhd` file. To generate an appropriate `eventTimes` for the class reading through these functions will be key. For the most up to date intan functions they offer their functions as a zip file although *if downloading from their website* I changed the import structure to fit with my pipeline (ie from intan_helpers.file import function)
 
## Importing

neuralanalysis can be imported by submodule or overall
```python
from neuralanalysis.ClusterAnalysis import ClusterAnalysis
```
or, 

```python
import neuralanalysis.full as na
```
