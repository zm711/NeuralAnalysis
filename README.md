# ClusterAnalysis
Pipeline for analyzing kilosort/phy data
some functions based off of Nick Steinmetz's matlab code, others original


## Requirements

I've tested python 3.8-3.10. I've also tested some other packages and these are my current to recreate. I work in spyder so I generate a conda env with its own spyder I had stability issues so I'm sticking with 5.3.3 for now. Using the packages below will prevent any compatibility issues.

```sh
 conda create -n spyderweb -c conda-forge python=3.10 spyder=5.3.3 numpy=1.23 pandas=1.5 scipy=1.10 matplotlib=3.63 h5py=3.8 seaborn=0.12 scikit-learn=1.12 cython=0.29 sympy=1.11 numba=1.23
 ```
 
 ## Inputs for the Class
 
 ### Neural Data
 This pipline is based specifically on kilosort/phy outputs. These include a series of numpy (.npy) files which are generated by kilosort and edited by phy. As long as phy is the final output then my `loadsp` function should load all data into a dictionary which I call `sp` for spike properties. Within the `ksanalysis.py` file all necessary keys can be found, but a few important ones include: `spikeTimes` are the list of spike occurrences in seconds, `clu` is the list of curated cluster ids for each spike, `cids` are the list of all possible ids after curation. 
 
 ### Stimulus Data
 Many *in vivo* experiments are based on recording neural activity in response to stimulus data. Stimulus data also needs to be loaded as a specific dictionary structure. Since I use Intan for recording data and stimuli, I use their python code for extracting stimulus data and generating this dictionary. If another data recording system is used the general format required is: eventTimes['stim channel']['EventTime']: np.array (of events), eventTimes['stim channel']['Length']: np.array (of lengths for each event), eventTimes['stim channel']['TrialGroup']: np.array of the different degree of stim (for example changing light orientation or changing pressure of stimuli) and eventTimes['stim channel']['Stim']: str ( the name of the stimulus for plotting). 
 
 #### Intan_helpers Folder
 Includes functions to run intan data automatically along with my functions for prepping the stimulus data from the .rhd file. To generate an appropriate `eventTimes` for the class reading through these functions will be key. For the most up to date intan functions they offer their functions as a zip file although of note if downloading from their website I changed the import structure to fit with my pipeline (ie from intan_helpers.file import function)
 
 ## Initialize the class
 The class is initialized by the spike data (sp) and the stimuli data (eventTimes).
 
 ```python
 from ClusterAnalysis import ClusterAnalysis
 myNeuron = ClusterAnalysis(sp, eventTimes)
 ```
 **I will use myNeuron as the name for an instance of `ClusterAnalysis` for the rest of this doc**
 
 ## First methods
 
 ### Setting important recording values
 There are a few values beneficial for this type of analysis which cannot be predicted when load the data. First trial groups are loaded as numeric values so likely for graphing it is necessary to map the numeric values to a stimulus value. This can be done with a dictionary. my_stim = {'1.0': '180 Degrees',..., '10.0': '370 Degrees'}. In addition the depth of the probe if measured can be factored into the analysis (500 um or 1000 um). Finally since most of the nervous system is bilateral indicating whether the recording was done of the 'l' or 'r' may be useful. So running the `set_labels` methods of `ClusterAnalysis` allows for inputting these values.
 
 ```python
 myNeuron.set_labels(labels = my_stim, depth = 1000, laterality='l')
 ```
 
 ### Generating raw waveform data
 Raw waveform data as opposed to the templates from phy can be beneficial for assessing peak-trough duration, amplitude etc. We can load this data from the `.bin` file that had been generated for kilosort based on our post-curation neural data. Of note this is a slow, RAM hungry process that performs a memory map of the binary file. It may need to be done on a server or a high-RAM workstation.
 
 ```python
 myNeuron.get_waveforms()
 ```
 
 ### Generating quality metrics
 Based on Nick Steinmetz's sortingQuality repo the `qcfn` method returns the isolation distance (Harris et al 2001) based on mahalobnis distance between pc values of clusters and the interspike interval violations (Schmitzer & Tobin et al. 2006). In short we can approximate the separation of units in our recording based on the distance in the pc spaces used by kilosort. Since Euclidean distance is prone to be influenced by less important features use of mahalobnis which uses the covariance matrix is less prone to these errors. ISI violations depend on the existence of neural refractory periods. 
 
 **There are no hard cutoffs for either value. Appropriate cutoffs must be determined based on your analysis**
 
 ```python
 myNeuron.qcfn()
 ```
 
 ## If loading a previous analysis
 Although the class requires `sp` and `eventTimes` for each initialization. There are load previous and save methods which can reduce the number of times methods must be run. Once waveform data and qcvalues have been generated files are saved and can be loaded for future analysis. This is accomplished with the `get_files` methods. Of note there is the `title` flag. This flag is to line up with the `title` flag in the `save_analysis` method. If some sort of subanalysis (for instance use of a different qc threshold) could be saved as separate files each given a unique title value.
 
 ```python
 myNeuron.get_files(title='')
 ```
 ## Analyzing Data
 
 All analysis is split amongst generating values which are stored as class attributes and plotting functions which use these attributes for plotting. But to do your own analyses based on these values just access the appropriate attributes (list at bottom of this document). Methods where I often inspect values return from the class, but most methods just store as attributes.
 
 ### Firing Rate Data
 
 Spike counts are the fundamental neural data for in vivo analysis. In order to generate these counts we need a `time_bin_size` given in seconds. 10-50 milliseconds work pretty well, but for slower neurons longer time bins provides more smoothing of the data and smaller time bins provides more 0 count bins. This function generates the `psthvalues` attribute of the `ClusterAnalysis` class which is organized as a dictionary of neurons with each neuron having a 'BinnedArray' with the matrix of firing rates give as an nEvents x nTimeBins.
 
 ```python
 psthvalues, windowlst = myNeuron.spike_raster(time_bin_size=0.05) # 50 millisecond example
 ```
 
 ### Z scored Data
 
 Data can also be z scored to allow for normalization of data. This requires that the std != 0, but to account for this an np.array called `normVal` is also produced which indicates the baseline mean and std or np.nan if un-z scoreable. *reminder Z score = x-mu/std* The return is `allP` a structure of the z scores stored as a dictionary of stimuli, followed by a matrix of the z score data. If `tg` is `True` then it will be nUnits x nTrialGroups x nTimeBins otherwise it will be nUnits x nTimeBins. `time_bin_size` like above is size of the time_bins. I also give an optional chance to input a `window_list` which is formatted as nested lists where each stimulus require two lists of time. Since this is complicated I explain below:
 
 #### Windows
 Each stmiulus needs the baseline period to generate the baseline mean and std. [bslStart, bslEnd]. These times are in relation to the event onset. So do to do the 2 seconds before the stimuli onsets would be [-2,0]. To do 500 to 100 milliseconds would be [-.5, -.1]. Then each stimulus also needs a window. Since some neurons have after-discharges this value can be longer or shorter than the actual stimlus. If I am doing a 5 second stim I could analyze the first 2 seconds [0, 2] or the last 2 [3, 5] or [0, 7] for full stimlus but the 2 seconds after.
 
 So the final window_list would be [[bslS, bslE], [start, end]] or with multiple different stimuli it's best to set to `None` and let the function prompt each stimuli itself
 ###
 
 Returns allP, the dictionary of z scores, normVal a dictionary with mean and std of the baseline/neuron or nan, window used for analysis. Attributes are `allP`, `normVal`, `zwindow`
 
 ```python
 allP, normVal, window= myNeuron.clu_zscore(time_bin_size = 0.05, tg=True, window=None)
 ```
